---
title: "Unsupervised learning - clustering and dimension reduction"
author: "Allen Zhu"
date: "Fall 2019 9/18/19"
output:
  html_document:
    df_print: paged
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = T)
```

# Lab Section
Download auto data from the *Statistical Learning* book website here: http://www-bcf.usc.edu/~gareth/ISL/data.html

Today, we are going over Hierarchical clustering, K-Means Clustering, PCA, and ICA. 

```{r load, include=FALSE}
library(ggplot2)
library(tidyverse)
library(ggfortify)
library(fastICA)
library(ggsci)
library(cluster)
```


## Homework

```{r}
data(iris)
```

0. Subset the Iris dataset to only include `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`. 

```{r}
vaars <- c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')
new_iris <- iris[vaars]
```




1. Write out the Kmeans algorithm by hand, and run two iterations of it. 

KMeans Skeleton Code:
First determine the number of clusters that needs to be tested. Since there are three species of iris in the dataset, k will be set to 3. 

If manually doing the algorithm by hand, my first 3 clusters will be 3 randomly selected points in the data set.

The euclidean distance is then calculated, and then the variables are sorted into the cluster that is closest to them (distance-wise).

The new centroids for each cluster can now be calculated by utilizing the mean for each variable within each cluster (in this case, sepal width, sepal height, and petal length), and the new euclidean distance can now be calculated for each variable.

The above step is repeated until the clustering becomes consistent, and subsequent iterations yield no changes. 

```{r}
# Set seed so the randomly generated values can be replicated
set.seed(15)

random_int1 <- sample(1:150, 1)
random_int2 <- sample(1:150, 1)
random_int3 <- sample(1:150, 1)


# Using the randomly generated numbers to select datapoints from within the set
NI1 <- new_iris[random_int1,]
NI2 <- new_iris[random_int2,]
NI3 <- new_iris[random_int3,]

```

```{r Euclidean Distance for First Iteration}
# Copy of original data into a new variable 
first_iris <- new_iris

# Calculate the euclidean distance for the three variables
first_iris <- mutate(first_iris, cluster1_1it = (NI1[,1]-Sepal.Length)**2+(NI1[,2]-Sepal.Width)**2 +(NI1[,3]-Petal.Length)**2 +(NI1[,4]-Petal.Width)**2)
first_iris <- mutate(first_iris, cluster2_1it = (NI2[,1]-Sepal.Length)**2+(NI2[,2]-Sepal.Width)**2 +(NI2[,3]-Petal.Length)**2 +(NI1[,4]-Petal.Width)**2)
first_iris <- mutate(first_iris, cluster3_1it = (NI3[,1]-Sepal.Length)**2+(NI3[,2]-Sepal.Width)**2 +(NI3[,3]-Petal.Length)**2 +(NI1[,4]-Petal.Width)**2)


```


```{r Sorting into Clusters}

# Sorting all of the variables into a defined cluster based on their smallest euclidean value. 
first_iris <- mutate(first_iris, first_it_cluster = ifelse((first_iris$cluster1_1it < first_iris$cluster3_1it & first_iris$cluster1_1it < first_iris$cluster2_1it) == TRUE, 1, ifelse((first_iris$cluster2_1it <  first_iris$cluster3_1it & first_iris$cluster2_1it < first_iris$cluster1_1it) == TRUE, 2, 3)))


# Counting the  number of samples in each cluster
length(first_iris$first_it_cluster[first_iris$first_it_cluster== 1])
length(first_iris$first_it_cluster[first_iris$first_it_cluster== 2])
length(first_iris$first_it_cluster[first_iris$first_it_cluster== 3])

# Filter the data frame based on clustering, and calculate the mean of each variable within each cluster.
t1 <- filter(first_iris, first_iris$first_it_cluster == 1)
t2 <- filter(first_iris, first_iris$first_it_cluster == 2)
t3 <- filter(first_iris, first_iris$first_it_cluster == 3)

# Finding out the new centroids for each cluster.
t1_sum <- t1[,1:4] %>% summarize_each(funs(mean))
t1_sum

t2_sum <- t2[,1:4] %>% summarize_each(funs(mean))
t2_sum

t3_sum <- t3[,1:4] %>% summarize_each(funs(mean))
t3_sum



first_iris
```


```{r Second Iteration}

# Copy original iris data-frame for the second iteration of the data
second_iris <- new_iris

# Calculate the new euclidean distance using the calculated centroids from before
second_iris <- mutate(second_iris, cluster1_2it = (t1_sum[,1]-Sepal.Length)**2+(t1_sum[,2]-Sepal.Width)**2 +(t1_sum[,3]-Petal.Length)**2 +(t1_sum[,4]-Petal.Width)**2)
second_iris <- mutate(second_iris, cluster2_2it = (t2_sum[,1]-Sepal.Length)**2+(t2_sum[,2]-Sepal.Width)**2 +(t2_sum[,3]-Petal.Length)**2 +(t2_sum[,4]-Petal.Width)**2)
second_iris <- mutate(second_iris, cluster3_2it = (t3_sum[,1]-Sepal.Length)**2+(t3_sum[,2]-Sepal.Width)**2 +(t3_sum[,3]-Petal.Length)**2 +(t3_sum[,4]-Petal.Width)**2)


second_iris

```


```{r}

# Sort each sample into clusters based on their euclidean values
second_iris <- mutate(second_iris, second_it_cluster = ifelse((second_iris$cluster1_2it < second_iris$cluster3_2it & second_iris$cluster1_2it < second_iris$cluster2_2it) == TRUE, 1, ifelse((second_iris$cluster2_2it <  second_iris$cluster3_2it & second_iris$cluster2_2it < second_iris$cluster1_2it) == TRUE, 2, 3)))

second_iris


# Count the amount of subjects in each cluster
length(second_iris$second_it_cluster[second_iris$second_it_cluster== 1])
length(second_iris$second_it_cluster[second_iris$second_it_cluster== 2])
length(second_iris$second_it_cluster[second_iris$second_it_cluster== 3])



# Sort the subjects by their cluster, and calculate the new means for each cluster.
t21 <- filter(second_iris, second_iris$second_it_cluster == 1)

t22 <- filter(second_iris, second_iris$second_it_cluster == 2)

t23 <- filter(second_iris, second_iris$second_it_cluster == 3)


# Calculate new centroids again.
t21_sum <- t1[,1:4] %>% summarize_each(funs(mean))
t21_sum

t22_sum <- t2[,1:4] %>% summarize_each(funs(mean))
t22_sum

t23_sum <- t3[,1:4] %>% summarize_each(funs(mean))
t23_sum


# Visualize entire dataframe
ggplot(data = second_iris) + geom_point(aes(Sepal.Length, Sepal.Width, color = as.character(second_it_cluster)))+ ggsci::scale_color_futurama()

```








2. Run PCA on the Iris dataset. Plot a scatter plot of PC1 vs PC2 and include the percent variance those PCs describe. 
## Principal Components Analysis (PCA)

Note that I did this question in 2 different ways.

First I did it as shown in the step by step process for me to better understand how PCA works. I then made the same plot using the prcomp.

```{r}
# Turning the data set into a data matrix.
iris_pca <- data.matrix(new_iris)
# Moving the data to be centered around the origin.

ggplot(data = second_iris) + geom_point((aes(Sepal.Length, Sepal.Width)))

Center_iris <- apply(iris_pca, 2, function(x) x - mean(x))
ggplot(data = Center_iris) + geom_point((aes(Sepal.Length, Sepal.Width)))

```


Step 2. Calculate covariance matrix of the Auto data

```{r}
#Caclulates the SSDistances 
Covariance_iris <- cov(Center_iris)
Covariance_iris
```

Step 3.  Calculate eigen values and vectors

```{r}
Eigen_value_iris <- eigen(Covariance_iris)$value

#columns are the eigen vectors
Eigen_vector_iris <- eigen(Covariance_iris)$vector
```

Step 4. Multiply the eigen vector matrix by the original data. 

```{r}
PC <- as.data.frame(data.matrix(Center_iris) %*% Eigen_vector_iris)

ggplot(PC, aes(PC[,1], PC[,2])) + geom_point(aes(PC[,1], PC[,2]))
#+ geom_text(aes(label=Auto_data_names[1:8]), nudge_x = -2.5, nudge_y = 400)
```

Step 5. Find out which principal components explain the variance in the data. 

```{r}
#for each component, take the cumulative sum of eigen values up to that point and and divide by the total sum of eigen values
round(cumsum(Eigen_value_iris)/sum(Eigen_value_iris) * 100, digits = 2)
```

```{r}
autoplot(prcomp(iris_pca))
```




3. Run ICA on the Iris dataset. Plot the independent components as a heatmap.

```{r}
iris_ica <- fastICA(iris_pca, 4, alg.typ = "parallel", fun = "logcosh", alpha = 1,
             method = "R", row.norm = FALSE, maxit = 200,
             tol = 0.0001, verbose = TRUE)

heatmap(iris_ica$S)

```





4. Use Kmeans to cluster the Iris data. 
  * Use the silhouette function in the cluster package to find the optimal number of clusters for kmeans for the iris dataset. Then cluster using kmeans clustering. Does the data cluster by species? 
  * Using this clustering, color the PCA plot according to the clusters.


After running a gap statistics test on the iris data, it stated that the optimal number of cluster s is 2. This is consistent with what the silouhette plot shows, with two distinct groups being shown. This means that the data clustering is either not based entirely on species, or that two of the species are similar enough to be considered a single cluster. Indeed, after creating a silhouette plot based on the flower species, we observe that two species are closely aligned to one another, and are sidered as a singular cluster.
  
  
```{r KMean Silhoutte Plot Colored by Cluster}  
set.seed(15)

clusGap(iris_pca, kmeans, K.max = 10, B=150)

km_iris <- kmeans(new_iris, 2, 150)  

dis_iris <- dist(new_iris)^2

sil_iris <- silhouette(km_iris$cluster, dis_iris)
plot(sil_iris, col = km_iris$cluster)

autoplot(prcomp(iris_pca), col=km_iris$cluster)
```



```{r KMean Silhoutte Plot Colored by Species}  
set.seed(15)


vaars1 <- c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width', 'Species')
test_iris <- iris[vaars1]

iris_df <- data.matrix(test_iris)

clusGap(iris_pca, kmeans, K.max = 10, B=150)

km_iris1 <- kmeans(new_iris, 2, 150)  
km_iris1$cluster

dis_iris1 <- dist(test_iris)^2

sil_iris1 <- silhouette(km_iris1$cluster, dis_iris1)
plot(sil_iris1, col = test_iris$Species)

```  



5. Use hierarchical clustering to cluster the Iris data.

  * Try two different linkage types, and two different distance metrics. 
  * For one linkage type and one distance metric, try two different cut points. 
  * Using this clustering, color the PCA plot according to the clusters. (6  plots in total)

```{r Euclidean and Centroid}
iris_dist1 <- dist(new_iris, method = "euclidean")
tree1 <- hclust(iris_dist1, method="centroid")
plot(tree1)

tree_k1 <- cutree(tree1, k = 3)
# plot the tree before running this line 
rect.hclust(tree1, k = 3, h = NULL)

tree_iris1 <- cbind(new_iris, tree_k1)



autoplot(prcomp(tree_iris1), col = tree_iris1$tree_k1)

```




```{r Euclidean and Average with cut height of 2}
iris_dist2 <- dist(new_iris, method = "euclidean")
tree2 <- hclust(iris_dist2, method="average")
plot(tree2)

tree_k2 <- cutree(tree2, k = 3, h = 2)
rect.hclust(tree2, h = 2)


tree_iris2 <- cbind(new_iris, tree_k2)



autoplot(prcomp(tree_iris2), col = tree_iris2$tree_k2)
```

```{r Euclidean and Average with cutpoint of 1}

tree22 <- hclust(iris_dist2, method="average")
plot(tree22)


tree_k22 <- cutree(tree22, h = 3)
rect.hclust(tree22, h = 1)


```





```{r Manhattan and Average with cut height of 3}
iris_dist4 <- dist(new_iris, method = "manhattan")
tree4 <- hclust(iris_dist4, method="average")
plot(tree4)

tree_k4 <- cutree(tree4, k = 3)
# plot the tree before running this line 
rect.hclust(tree1, h = 3)

tree_iris4 <- cbind(new_iris, tree_k4)



autoplot(prcomp(tree_iris4), col = tree_iris4$tree_k4)

```


```{r Manhattan and Average with cut height of 1}
tree42 <- hclust(iris_dist2, method="average")
plot(tree42)


tree_k22 <- cutree(tree22, h = 1)
rect.hclust(tree22, h = 1)


```





```{r Manhattan and Centroid}
iris_dist6 <- dist(new_iris, method = "manhattan")
tree6<- hclust(iris_dist6, method="centroid")
plot(tree6)

tree_k6 <- cutree(tree6, k = 3)
# plot the tree before running this line 
rect.hclust(tree6, k = 3, h = NULL)

tree_iris6 <- cbind(new_iris, tree_k6)


autoplot(prcomp(tree_iris6), col = tree_iris6$tree_k6)

```





